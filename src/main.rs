use base64::{engine::general_purpose::STANDARD, Engine};
use chrono::Utc;
use log::{error, info, LevelFilter};
use ollama_rs::{
    generation::{completion::request::GenerationRequest, images::Image},
    Ollama,
};
use poise::{
    serenity_prelude::{self as serenity, CreateAttachment, CreateEmbed, CreateEmbedFooter},
    CreateReply,
};

struct Data {} // User data, which is stored and accessible in all command invocations
type Error = Box<dyn std::error::Error + Send + Sync>;
type Context<'a> = poise::Context<'a, Data, Error>;

#[derive(Debug, poise::ChoiceParameter)]
pub enum LLMModels {
    #[name = "mistral"]
    Mistral,
    #[name = "caveman"]
    Caveman,
    #[name = "racist"]
    Racist,
    #[name = "lobotomy"]
    Lobotomy,
    #[name = "greentext"]
    Greentext,
}

impl std::fmt::Display for LLMModels {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            LLMModels::Mistral => write!(f, "dolphin-mistral"),
            LLMModels::Caveman => write!(f, "caveman-mistral"),
            LLMModels::Racist => write!(f, "racist-mistral"),
            LLMModels::Lobotomy => write!(f, "qwen:0.5b-chat-v1.5-q2_K"),
            LLMModels::Greentext => write!(f, "greentext-mistral"),
        }
    }
}

#[derive(Debug, poise::ChoiceParameter)]
pub enum ImageModels {
    #[name = "SDXLTurbo"]
    SDXLTurbo,
    #[name = "StableCascade (SLOW)"]
    StableCascade,
}

impl std::fmt::Display for ImageModels {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            ImageModels::SDXLTurbo => write!(f, "SDXL Turbo"),
            ImageModels::StableCascade => write!(f, "Stable Cascade"),
        }
    }
}

/// Generates a text reponse using the specified model and prompt
#[poise::command(slash_command, prefix_command, user_cooldown = 10)]
async fn llm(
    ctx: Context<'_>,
    #[description = "Model"] model: LLMModels,
    #[description = "Prompt"] prompt: String,
) -> Result<(), Error> {
    ctx.defer().await?;
    info!("Generating response for model `{model}` and prompt `{prompt}`...");
    let ollama = Ollama::default();
    let res = ollama
        .generate(GenerationRequest::new(model.to_string(), prompt.clone()))
        .await;

    match res {
        Ok(response) => {
            let footer = CreateEmbedFooter::new("Made by @DuckyBlender | Generated with Ollama");
            // token count / duration
            let response_speed = f32::from(response.final_data.clone().unwrap().eval_count)
                / (response.final_data.clone().unwrap().eval_duration as f32 / 1_000_000_000.0);
            let res = response.response.trim();

            let description = format!("**Prompt:**\n{}\n\n**Response:**\n{}", prompt, res);

            let embed = CreateEmbed::default()
                .title(format!("Generated by `{model}`"))
                .color(0x00ff00)
                .description(description)
                .field(
                    "Duration",
                    format!(
                        "`{:.2}s`",
                        // total_durations is in nanoseconds
                        response.final_data.clone().unwrap().total_duration as f32
                            / 1_000_000_000.0
                    ),
                    true,
                )
                .field("Speed", format!("`{response_speed:.2} tokens/s`"), true)
                .footer(footer)
                .timestamp(Utc::now());
            let message = CreateReply::default().embed(embed);
            ctx.send(message).await?;
            info!("Response sent successfully");
        }
        Err(e) => {
            let embed = CreateEmbed::default()
                .title("Error generating response")
                .description(format!("Error: {e}"))
                .color(0xff0000)
                .timestamp(Utc::now());
            let message = CreateReply::default().embed(embed);
            ctx.send(message).await?;
            error!("Failed to generate response: {:?}", e);
        }
    }

    Ok(())
}

#[poise::command(slash_command, prefix_command, user_cooldown = 10)]
async fn clone_image(
    ctx: Context<'_>,
    #[description = "Image to clone"] image_url: String,
) -> Result<(), Error> {
    ctx.defer().await?;

    // Get the image and save it to a buffer
    let image = reqwest::get(&image_url).await?.bytes().await?;
    // Convert image to base64
    let image = STANDARD.encode(image);

    info!("Getting prompt from image...");
    let start = std::time::Instant::now();
    let ollama = Ollama::default();
    let res = ollama
        .generate(
            GenerationRequest::new(
                "llava:latest".to_string(),
                "Describe this image in one sentence.".to_string(),
            )
            .images(vec![Image::from_base64(&image)]),
        )
        .await;
    let generated_now = std::time::Instant::now();

    // Response
    let res = res.unwrap().response;
    let res = res.trim();
    // Generate the image using the llava response
    let client = comfyui_rs::Client::new("127.0.0.1:8188");
    let json_prompt = include_str!("../jsons/sdxl_turbo_api.json");
    // Convert to JSON value
    let mut json_prompt: serde_json::Value = serde_json::from_str(json_prompt).unwrap();
    // Change the prompt
    json_prompt["6"]["inputs"]["text"] = serde_json::Value::String(res.to_string());
    json_prompt["13"]["inputs"]["noise_seed"] =
        serde_json::Value::Number(serde_json::Number::from(rand::random::<u64>()));
    // json_prompt["22"]["inputs"]["steps"] =
    //     serde_json::Value::Number(serde_json::Number::from(steps));
    let images = client.get_images(json_prompt).await.unwrap();
    let end_now = std::time::Instant::now();
    info!(
        "Image generated successfully in {elapsed}ms",
        elapsed = (end_now - start).as_millis()
    );
    // Send this as an attachment
    // let attachment = CreateAttachment::bytes(image, "crong.png");
    let attachments = images
        .iter()
        .map(|(filename, bytes)| CreateAttachment::bytes(bytes.clone(), filename))
        .collect::<Vec<_>>();

    // For now just send the first image (because we're generating one image)
    // I'm not sure if it's even possible to send multiple images in a single message
    let footer = CreateEmbedFooter::new(format!(
        "Made by @DuckyBlender | Generated with LLaVA -> SDXL-Turbo"
    ));
    let message = CreateReply::default()
        .attachment(attachments[0].clone())
        .embed(
            CreateEmbed::default()
                .title("SDXL-Turbo")
                .fields(vec![
                    ("Prompt", format!("`{res}`"), true),
                    (
                        "Duration",
                        format!(
                            "`{:.2}s + {:.2}s = {:.2}s`",
                            (generated_now - start).as_secs_f32(),
                            (end_now - generated_now).as_secs_f32(),
                            (end_now - start).as_secs_f32()
                        ),
                        true,
                    ),
                    // ("Steps", format!("`{steps}`"), true),
                ])
                .color(0x00ff00)
                .footer(footer)
                .timestamp(Utc::now()),
        );
    ctx.send(message).await?;
    info!("Image sent successfully");

    Ok(())
}

/// Generates an image using the specified model and prompt
#[poise::command(slash_command, prefix_command, user_cooldown = 10)]
async fn img(
    ctx: Context<'_>,
    #[description = "Model"] model: ImageModels,
    #[description = "Prompt"] prompt: String,
) -> Result<(), Error> {
    info!("Generating image for prompt `{prompt}`...");
    ctx.defer().await?;
    let client = comfyui_rs::Client::new("127.0.0.1:8188");
    // Convert to JSON value
    let mut json_prompt: serde_json::Value;
    match model {
        ImageModels::SDXLTurbo => {
            json_prompt =
                serde_json::from_str(include_str!("../jsons/sdxl_turbo_api.json")).unwrap();
            json_prompt["6"]["inputs"]["text"] = serde_json::Value::String(prompt.clone());
            json_prompt["13"]["inputs"]["noise_seed"] =
                serde_json::Value::Number(serde_json::Number::from(rand::random::<u64>()));
        }
        ImageModels::StableCascade => {
            json_prompt =
                serde_json::from_str(include_str!("../jsons/stable_cascade_api.json")).unwrap();
            json_prompt["6"]["inputs"]["text"] = serde_json::Value::String(prompt.clone());
            json_prompt["3"]["inputs"]["seed"] =
                serde_json::Value::Number(serde_json::Number::from(rand::random::<u64>()));
            json_prompt["33"]["inputs"]["seed"] =
                serde_json::Value::Number(serde_json::Number::from(rand::random::<u64>()));
        }
    }

    let now = std::time::Instant::now();
    let images = client.get_images(json_prompt).await.unwrap();
    let elapsed = now.elapsed().as_millis();
    info!("Image generated successfully in {elapsed}ms using {model}");
    // Send this as an attachment
    // let attachment = CreateAttachment::bytes(image, "crong.png");
    let attachments = images
        .iter()
        .map(|(filename, bytes)| CreateAttachment::bytes(bytes.clone(), filename))
        .collect::<Vec<_>>();

    // For now just send the first image (because we're generating one image)
    // I'm not sure if it's even possible to send multiple images in a single message
    let footer = CreateEmbedFooter::new(format!("Made by @DuckyBlender | Generated with {model}"));
    let message = CreateReply::default()
        .attachment(attachments[0].clone())
        .embed(
            CreateEmbed::default()
                .title(model.to_string())
                .fields(vec![
                    ("Prompt", format!("`{prompt}`"), true),
                    (
                        "Duration",
                        format!("`{:.2}s`", elapsed as f32 / 1000.0),
                        true,
                    ),
                    // ("Steps", format!("`{steps}`"), true),
                ])
                .color(0x00ff00)
                .footer(footer)
                .timestamp(Utc::now()),
        );
    ctx.send(message).await?;
    info!("Image sent successfully");

    Ok(())
}

/// Gets the GPU stats
// #[poise::command(slash_command, prefix_command, user_cooldown = 1)]
// async fn stats(ctx: Context<'_>) -> Result<(), Error> {
//     info!("Getting stats...");
//     let client = comfyui_rs::Client::new("127.0.0.1:8188");
//     let stats: SystemStats = client.get_system_stats().await.unwrap();
//     let embed = CreateEmbed::default()
//         .title("Stats")
//         .fields(vec![
//             (
//                 "GPU",
//                 format!(
//                     "{}\nVRAM: {}MB/{}MB",
//                     stats.devices[0].name,
//                     stats.devices[0].vram_free / 1_000_000, // convert to MB
//                     stats.devices[0].vram_total / 1_000_000  // convert to MB
//                 ),
//                 true,
//             ), // todo add more fields from /queue
//         ])
//         .color(0x00ff00)
//         .timestamp(Utc::now());
//     let message = CreateReply::default().embed(embed);
//     ctx.send(message).await?;
//     info!("Stats sent successfully");

//     Ok(())
// }

#[tokio::main]
async fn main() {
    dotenv::dotenv().ok();
    // Initialize env_logger with a custom configuration
    let mut builder = env_logger::Builder::new();
    builder.filter(None, LevelFilter::Warn); // Set the default level to Warn for all modules
    builder.filter(Some("duckgpt"), LevelFilter::Info);
    builder.filter(Some("comfyui_rs"), LevelFilter::Info);
    builder.init();

    let token = std::env::var("DISCORD_TOKEN").expect("missing DISCORD_TOKEN");
    let intents = serenity::GatewayIntents::non_privileged();

    let framework = poise::Framework::builder()
        .options(poise::FrameworkOptions {
            commands: vec![llm(), img(), clone_image()],
            ..Default::default()
        })
        .setup(|ctx, _ready, framework| {
            Box::pin(async move {
                let guild_id = serenity::GuildId::new(1175184892225671258);
                poise::builtins::register_in_guild(ctx, &framework.options().commands, guild_id)
                    .await?;
                Ok(Data {})
            })
        })
        .build();

    let client = serenity::ClientBuilder::new(token, intents)
        .framework(framework)
        .await;
    client.unwrap().start().await.unwrap();
}
